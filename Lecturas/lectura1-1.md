# Comentario: "Item-Based Collaborative Filtering Recommendation Algorithms" (2001)

El paper se basa en los dos desafíos principales a los que se enfrentan los sistemas recomendadores que usan _Collaborative Filtering_ a medida que la cantidad de información disponible aumenta. El primer desafío es la **escalabilidad**, que refiere a que el tiempo que tardan los algoritmos aumenta de manera significativa al aumentar la información, ya que aumenta la cantidad de vecinos potenciales a recorrer, y la cantidad de información relacionada a cada usuario. El segundo desafío sería mejorar la **calidad** de las recomendaciones, ya que un usuario promedio se rehusará a usar algoritmos que no acierten de forma regular.

Como respuesta a estos dos desafíos, los autores proponen hacer un cambio en los algoritmos, reemplazando la elaboración de relaciones entre usuarios por la elaboración de relaciones entre ítems a recomendar. Proponen un modelo óptimo de _Item-Based Collaborative Algorithm_, explicando la justificación de cada componente y parámetro del modelo, y luego lo comparan con otros modelos, tanto del tipo _item-based_ como _user-based_ en términos de rendimiento y velocidad. Como resultado obtienen que el modelo del tipo _item-Based_ sin regresión lineal es el con mejor rendimiento, tiempo de respuesta y _throughput_, demostrando lo prometedores que son estos tipos de algoritmos.

Entre los aspectos que me parecieron más interesantes, se encuentra el proceso de _Collaborative Filtering_, en específico el proceso de generar _ratings_/opiniones a partir de datos implícitos. Buscando en los escritos a los que el paper hace referencia al momento de hablar de los _rankings_ implícitos, encontre el escrito ["Applying Collaborative Filtering to Usenet News"](https://pdfs.semanticscholar.org/b748/7df74dd2d838fdc553a477f46ca5a502fb04.pdf) del mismo grupo de investigación de GroupLens donde se explica que ciertas predicciones implícitas, por ejemplo, el tiempo que un usuario estuvo leyendo un artículo eran casi tan certeras como predicciones basadas en _rankings_ explícitos. 

Otro punto que me parece interesante también proviene del _Collaborative Filtering_, y es el problema de la escasez en los datos. En específico, el paper menciona el uso de _Filtering Agents_ y posteriormente _Latent Semantic Indexing_. Nuevamente, existe un paper de GroupLens ["Using Filtering Agents to Improve Prediction Quality in the Group Lens Research Collaborative Filtering System"](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.40.653&rep=rep1&type=pdf) el cual entra en más detalle sobre los _Filtering Agents_, donde el grupo propone el uso de unos _FilterBots_, que son robots automatizados que evaluan documentos nuevos tan pronto como son publicados e ingresan dicha evaluación al sistema. Luego, dependiendo de si un usuario está consistentemente de acuerdo o desacuerdo con los _ratings_ de cierto _FilterBot_, las evaluaciones de dicho _bot_ son usadas con mayor o menor frecuencia dentro de las recomendaciones a dicho usuario. Con respecto al _Latent Semantic indexing_, este concepto trata acerca de identificar cuando algún grupo de palabras claves o escritos apuntan al mismo significado, con el fin de recuperar todos los documentos o información relevante a un tema aunque no incluyan las mismas palabras claves exactas. En el _paper_ citado en esa sección ["Indexing by Latent Semantic Analysis"](http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf), se habla de que este método es prometedor para la obtención de información, algo que es necesario al momento de tratar de superar la escasez de los datos del dataset, ya que podemos hacer asociaciones de usuarios que piensan similar pero poseen pocas evaluaciones propias. 

Con respecto a críticas al _paper_, sentí que la sección donde se escoge la métrica bajo la cual evaluar los algoritmos propuestos vs. la _benchmark_ termina de forma muy abrupta. Dentro de las _Statistical Accuracy Metrics_ no se explica porque se prefiere MAE en vez de RMSE o _Correlation_. Incluso, estas dos últimas solo se nombran, y no son descritas ni diferenciadas de MAE breviamente. Dentro de las _Decision support accuracy Metrics_, tampoco se explican las diferencias entre ROC, _reversal rate_ y _weighted errors_, ni cual sería la mejor a usar en este caso. Finalmente, se elige MAE como la única métrica de evaluación para los algoritmos, y como esta fue la única explicada en detalle, sentí que no tenía toda la información necesaria para estar de acuerdo/en contra con la decisión de los autores sin tener que investigar de forma previa otros escritos.

De forma general, consideré el paper bastante interesante y sentí que logra explicar el razonamiento detrás de la ventaja en rendimiento que tienen los _item-based algorithms_ vs. los _user-based_, además de ayudar a entender el concepto de _Collaborative Filtering._