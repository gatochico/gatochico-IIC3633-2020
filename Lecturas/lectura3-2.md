# Comentario "Chapter 8: Evaluating Recommendation Systems" (2011)

Esta lectura corresponde a un capítulo del libro "Recommender Systems Handbook", y está dedicado exclusivamente a explicar cómo comparar el rendimiento de dos o más modelos, cómo evaluar si un modelo fue o no efectivo, y qué tipo de _tests_ existen para los sistemas recomendadores y cuándo usar cada uno.

Los autores mencionan tres tipos de experimentos a usar: _Offline_, _User Studies_ y _Online_. Los experimentos *_Offline_* corresponden a usar un dataset ya recolectado de usuarios evaluando o eligiendo _items_ y luego tratar de simular el comportamiento de dichos usuarios mediante un _test_ dataset. Se podría decir que los prácticos o tareas que realizamos en el curso son un tipo de _Offline_ test, al trabajar con datasets entregados.

Luego, se encuentran los experimentos *_User Studies_*, que refieren a conseguir personas o sujetos de prueba para que prueben los modelos bajo supervisión nuestra. De esta forma, se pueden obtener varios datos respecto a la interacción del usuario con el sistema. El concepto de _User Studies_ conlleva un análisis completo para su funcionamiento correcto, por lo que los autores solo muestran una idea general del concepto y proponen la búsqueda de información para quienes estén interesados.

Finalmente, se encuentran los experimentos _Online_: Estos buscan implementar el modelo y observar el comportamiento de los usuarios reales del sistema, prestando atención a posibles cambios causados por las recomendaciones entregadas por el modelo.

Luego, tras presentar los tipos de experimentos disponibles, el capítulo habla soble propiedades que poseen los sistemas recomendadores y que deben ser consideradas al momento de evaluar o comparar un modelo.

Hubieron dos puntos que me parecieron de particular interés en el texto y ambos refieren a propiedades de los sistemas recomendadores: El primero corresponde a la propiedad *Novelty* que refiere a que el sistema recomiende _items_ que el usuario no conocía de forma previa. Esto se debe a todo el análisis que se hace para disminuir la probabilidad de que una recomendación sea algo que el usuario ya haya visto. Dentro de los ejemplos propuestos, uno nace a partir de resultados obtenidos en un _User Study_ (Lo que ayuda a "conectar", de cierta forma, las dos secciones principales del capítulo, que serían los experimentos y las propiedades) y es que los usuarios tienen a no evaluar ciertos items que sí vieron, siguiendo un patrón. Por lo tanto, un arreglo que se le hace al modelo es "sacar" los items cuyo rating siga este patrón. Otros ejemplos incluyen este [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.9683&rep=rep1&type=pdf) llamado "Improving Recommendation Lists Through Topic Diversification" (2005), el cual tiene como fin proponer un método para diversificar y balancear recomendaciones. Este mismo paper indica como una de las métricas a tomar en cuenta más alla de la _accuracy_, está la _Novelty_ y la _Serendipity_, y obtienen como resultado un modelo que a pesar de tener menor _Accuracy_, entrega una mayor satisfacción por parte del usuario, comprobando que la necesidad de mostrar items nuevos a recomendar es algo que sí debe de tomarse en cuenta.

El otro punto refiere a la propiedad de *Robustness* (estabilidad del sistema en presencia de información falsa). Este punto es algo que todavía no se ha tocado en el curso de forma directa, pero que es de vital importancia a medida que la tecnología avanza. Aunque se admite que la posibilidad de crear un sistema que sea inmune a este tipo de ataques no es realista, es importante el estudio de estos posibles ataques. Por ejemplo, en el siguiente [paper](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.329.4891&rep=rep1&type=pdf) de nombre "Towards Trustworthy Recommender Systems: An Analysis of Attack Models and Algorithm Robustness" (2007), se analiza diferentes modelos de ataque y su efectividad contra los sistemas, mientras al mismo tiempo descubre que ciertos modelos son más robustos que otros frente a un ataque. Estos tipos de hallazgos permiten una mejor y más rápida neutralización del problema, disminuyendo los posibles costos que implique un ataque sobre el sistema. Considero que es una propiedad bastante interesante, ya que es un riesgo posible para los sistemas recomendadores una vez implementados.

En general, consideré esta lectura bastante interesante y que está bien ubicada dentro del programa del curso: Digo esto ya que con las lecturas pasadas ya se tiene un concepto sobre cómo funciona un modelo de recomendación, y durante las clases y prácticos ya se han visto distintos algoritmos a usar. Por lo tanto, es lógico ahora pensar en el proceso de evaluación/seleccion de modelos para una tarea y saber más sobre los detalles de este. Al incluirse de forma relativamente 'temprana' en el curso, esto también permite tener los conceptos de esta lectura en mente para lo que queda del curso y que no sea un _afterthought_, sino algo que tomar en cuenta de forma paralela mientras se estudian los modelos en sí.

## Referencias
Towards Trustworthy Recommender Systems: An Analysis of Attack Models and Algorithm Robustness. Mobasher, Burke, Bhaumik, Williams. (2007).
Improving Recommendation Lists Through Topic Diversification. Ziegler, McNee, Konstan, Lausen (2005).
